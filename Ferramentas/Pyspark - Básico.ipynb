{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0906ad8-10b4-4569-8264-5600c2eba12d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sessão Spark\n",
    "- Permite acessar o cluster spark.\n",
    "\n",
    ".builder() - \"Seta\" a sessão.  \n",
    ".getOrCreate() - Cria ou pega a sessão.  \n",
    ".appName() Ajuda no gerenciamento de múltiplas sessões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9db5d9ea-9d21-4839-8e0e-3d5500bf6995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criando uma sessão com spark\n",
    "from pyspark.sql import SparkSession\n",
    "# Inicializando a sessão\n",
    "spark = SparkSession.builder.appName(\"SparkSession\").getOrCreate()\n",
    "\n",
    "# Criando um dataframe\n",
    "''' df = spark.read.csv(\"Workspace/Monthly_Transportation_Statistics.csv\", header=True, inferSchema=True) '''\n",
    "# Nesse caso o databricks não possui acesso para ler arquivos csv com o pyspark\n",
    "\n",
    "# Mostrando o dataframe\n",
    "''' display(df) '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ee85f8-30cb-4994-86d2-e2e012812bd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dataframes\n",
    "- A maior diferença é a distribuição dos dados. O pandas possui uma única instância de computação enquanto o pyspark distribui os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79dd0b6b-e950-4f4b-8e94-63d615723ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "               select * \n",
    "               from workspace.bronze.b3_hist\n",
    "               where datetime >= date_sub(current_date,5)\n",
    "               \"\"\")\n",
    "\n",
    "print(df.count()) # Contagem de linhas\n",
    "\n",
    "display(df.limit(5)) # Mostrar tabela\n",
    "\n",
    "df.printSchema() #Mostrar a estrutura do df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad9de3b-e6ce-470e-8a78-d8ceada01ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Principais funções do pyspark:\n",
    ".select()  \n",
    ".filter() ou .where()  \n",
    ".groupBy()  \n",
    ".agg()  \n",
    ".sort()  \n",
    ".na.drop() --> Remoção de valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d64622b-1b11-4348-b18c-bab03d49d08f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Agrupamento e operações\n",
    "display(df.groupBy('Ticker').agg({'close':'avg'}).limit(2))\n",
    "\n",
    "display(df.filter(df['close']<5.0).select('ticker','close','datetime').limit(2))\n",
    "''' \n",
    "Outras operações:\n",
    "sum()\n",
    "min()\n",
    "max()\n",
    "count() \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a58ae2b-836a-4448-a5b7-15ecc0e343d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lidando com dados faltando / valores nulos\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.na.drop() \n",
    "df.where(col('close').isNotNull())\n",
    "\n",
    "df.na.fill({'close':0}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f8d0d0d-bd12-4e1d-925d-e1abc6bc2e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Operações com colunas\n",
    "\n",
    "# Renomear coluna\n",
    "df.withColumnRenamed('close','fechamento')\n",
    "\n",
    "# Adicionar uma coluna\n",
    "df = df.withColumn(\"dff_close_open\", df['close'] - df['open'])\n",
    "\n",
    "# Removendo colunas \n",
    "df = df.drop('dif_close_open')\n",
    "\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54af60cb-5db1-4be3-b732-3106b93e1e39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tipos de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d9a703a-e435-47b3-897e-ce211895727c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType,\n",
    "                               StructField, \n",
    "                               LongType, \n",
    "                               IntegerType,\n",
    "                               StringType,\n",
    "                               ArrayType\n",
    "                            )\n",
    "\n",
    "# Construção da estrutura do Dataframe\n",
    "schema = StructType([\n",
    "    StructField(\"ticker\", StringType(), True),\n",
    "    StructField(\"close\", LongType(), True),\n",
    "    StructField(\"precos\", ArrayType(IntegerType()), True)\n",
    "])\n",
    "\n",
    "dados = [\n",
    "    ('AAPL', 1.4, [1,2,3]),\n",
    "    ('GOOG', 25.4, [4,5,6])\n",
    "    ]\n",
    "df2 = spark.createDataFrame(dados, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0a464ff-d754-42be-9fdf-6864ed1cd3bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Operações avançadas\n",
    "- Join\n",
    "- Union\n",
    "- Arrays e Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5ddaca-5939-4290-a19c-245e8947b92d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join\n",
    "\n",
    "''' \n",
    "df_join = df1.join(df2,on=\"id\",how=\"inner\")\n",
    "\n",
    "df_join = df1.join(\n",
    "    df2,\n",
    "    df1.id == df2.row_id,\n",
    "    \"inner\"\n",
    ") \n",
    "'''\n",
    "\n",
    "# Union\n",
    "''' df_union = df1.union(df2) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55774c13-f843-47af-a9c1-1e59b8888df6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Trabalhando com arrays\n",
    "from pyspark.sql.functions import array, struct, lit\n",
    "\n",
    "df_array = df.withColumn(\"preco\",\n",
    "                         array(lit(85),\n",
    "                               lit(90),\n",
    "                               lit(78)\n",
    "                               )\n",
    "                         )\n",
    "display(df_array.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d0e8fc-5a9c-4433-94bd-2a81735238e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Trabalhando com maps\n",
    "from pyspark.sql.types import StructField, StructType,StringType, MapType \n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ticker\", StringType(), True),\n",
    "    StructField(\"close\", LongType(), True),\n",
    "    StructField(\"precos\", MapType(StringType(), LongType()), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6e7fca8-4655-4444-861c-8788025a94cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Trabalhando com structs\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "df_map = df.withColumn(\"name_struct\", struct(\"ticker\", \"datetime\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark - Básico",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
